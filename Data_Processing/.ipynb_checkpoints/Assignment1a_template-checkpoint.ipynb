{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skfuzzy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KMeans\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixture\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GaussianMixture, BayesianGaussianMixture\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mskfuzzy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mfuzz\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Statistical tools\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multivariate_normal\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'skfuzzy'"
     ]
    }
   ],
   "source": [
    "# this cell imports the libraries or packages that you can use during this assignment\n",
    "# you are not allowed to import additional libraries or packages\n",
    "\n",
    "from helpers import *\n",
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning and Decomposition\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture, BayesianGaussianMixture\n",
    "import skfuzzy as fuzz\n",
    "\n",
    "# Statistical tools\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy import linalg\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from matplotlib.colors import ListedColormap, LogNorm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Important**\n",
    ">\n",
    "> Do not import any other packages or libraries than the ones already provided to you. You can only use the imported packages _after_ they have been imported.\n",
    ">\n",
    "> Write your code between the `BEGIN_TODO` and `END_TODO` markers. Do not change these markers.\n",
    ">\n",
    "> Restart your notebook and run all cells before submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this assignment you will carry on data collection and preprocessing tasks, After collecting and preprocessing sensor recording, you will extract a set of features which you will use for creating the biking lane detector in assignment 1b.\n",
    "\n",
    "This assignment is split into 4 parts. Parts 1-3 encompass the collection, preprocessing and feature extraction of sensor recordings. In part 4 you will employ PCA and ICA for data compression.\n",
    "\n",
    "### Learning goals\n",
    "After this assignment you can\n",
    "- collect data according to a protocol;\n",
    "- load and merge datasets;\n",
    "- preprocess data;\n",
    "- extract features from data;\n",
    "- Perform data compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the data according to the data collection protocol provided on Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data cleaning and preprocessing\n",
    "In this part you will be working on cleaning and preprocessing the overall data that each group have gathered for further analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 2.1: Read, merge and select data\n",
    "Place your recordings in folder titled Data, inside the folder, create two folders, Bumpy folder and smooth folder, Load one of your individuals recordings from either folders in a Pandas DataFrame called `data`. You may want to have a look at the `pd.merge_asof` function to combine the recordings of the different sensors. Make sure that the `data` dataframe does not contain any `NaN`'s or empty fields as a result of different sampling frequencies. Any columns/recordings that you will not be using in your experiment should be removed from `data` (except for the `seconds_elapsed` column). Also remove duplicate colums. In the end your dataframe should have an indexing column, a column called `seconds_elapsed`, followed by the columns corresponding to the measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seconds_elapsed</th>\n",
       "      <th>z_accelerometer</th>\n",
       "      <th>y_accelerometer</th>\n",
       "      <th>x_accelerometer</th>\n",
       "      <th>z_gravity</th>\n",
       "      <th>y_gravity</th>\n",
       "      <th>x_gravity</th>\n",
       "      <th>z_gyroscope</th>\n",
       "      <th>y_gyroscope</th>\n",
       "      <th>x_gyroscope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.018845</td>\n",
       "      <td>-0.462222</td>\n",
       "      <td>-0.368679</td>\n",
       "      <td>-0.634270</td>\n",
       "      <td>-9.118624</td>\n",
       "      <td>-3.603003</td>\n",
       "      <td>-0.198613</td>\n",
       "      <td>0.103786</td>\n",
       "      <td>-0.146539</td>\n",
       "      <td>0.358608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.028829</td>\n",
       "      <td>-0.605616</td>\n",
       "      <td>-0.620192</td>\n",
       "      <td>-0.543474</td>\n",
       "      <td>-9.102573</td>\n",
       "      <td>-3.641936</td>\n",
       "      <td>-0.223269</td>\n",
       "      <td>0.061494</td>\n",
       "      <td>-0.329839</td>\n",
       "      <td>0.508239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.038814</td>\n",
       "      <td>-0.822008</td>\n",
       "      <td>-0.454557</td>\n",
       "      <td>-0.869079</td>\n",
       "      <td>-9.078464</td>\n",
       "      <td>-3.699083</td>\n",
       "      <td>-0.262031</td>\n",
       "      <td>0.021028</td>\n",
       "      <td>-0.478320</td>\n",
       "      <td>0.774517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.048798</td>\n",
       "      <td>-1.715980</td>\n",
       "      <td>-0.542211</td>\n",
       "      <td>-1.278268</td>\n",
       "      <td>-9.041318</td>\n",
       "      <td>-3.785458</td>\n",
       "      <td>-0.308639</td>\n",
       "      <td>0.026203</td>\n",
       "      <td>-0.532712</td>\n",
       "      <td>1.140537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.058782</td>\n",
       "      <td>-2.304195</td>\n",
       "      <td>-0.044734</td>\n",
       "      <td>-1.466343</td>\n",
       "      <td>-8.988506</td>\n",
       "      <td>-3.905100</td>\n",
       "      <td>-0.356842</td>\n",
       "      <td>0.082989</td>\n",
       "      <td>-0.477392</td>\n",
       "      <td>1.519668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5156</th>\n",
       "      <td>51.499675</td>\n",
       "      <td>-0.606824</td>\n",
       "      <td>0.025912</td>\n",
       "      <td>-0.293244</td>\n",
       "      <td>-8.390137</td>\n",
       "      <td>-5.030543</td>\n",
       "      <td>0.685295</td>\n",
       "      <td>0.013766</td>\n",
       "      <td>0.387161</td>\n",
       "      <td>0.390092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5157</th>\n",
       "      <td>51.509660</td>\n",
       "      <td>-0.618310</td>\n",
       "      <td>0.025874</td>\n",
       "      <td>-0.135084</td>\n",
       "      <td>-8.370122</td>\n",
       "      <td>-5.060581</td>\n",
       "      <td>0.708495</td>\n",
       "      <td>-0.010532</td>\n",
       "      <td>0.170958</td>\n",
       "      <td>0.322333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5158</th>\n",
       "      <td>51.519644</td>\n",
       "      <td>-0.727742</td>\n",
       "      <td>-0.061432</td>\n",
       "      <td>0.085160</td>\n",
       "      <td>-8.356308</td>\n",
       "      <td>-5.082959</td>\n",
       "      <td>0.711361</td>\n",
       "      <td>-0.032190</td>\n",
       "      <td>-0.140191</td>\n",
       "      <td>0.219038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5159</th>\n",
       "      <td>51.529629</td>\n",
       "      <td>-0.635359</td>\n",
       "      <td>-0.233826</td>\n",
       "      <td>0.323464</td>\n",
       "      <td>-8.348583</td>\n",
       "      <td>-5.098061</td>\n",
       "      <td>0.693773</td>\n",
       "      <td>-0.017748</td>\n",
       "      <td>-0.272743</td>\n",
       "      <td>0.157781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5160</th>\n",
       "      <td>51.539613</td>\n",
       "      <td>-0.205602</td>\n",
       "      <td>-0.248753</td>\n",
       "      <td>0.272006</td>\n",
       "      <td>-8.342745</td>\n",
       "      <td>-5.109919</td>\n",
       "      <td>0.676547</td>\n",
       "      <td>0.010838</td>\n",
       "      <td>-0.121989</td>\n",
       "      <td>0.129650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5161 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      seconds_elapsed  z_accelerometer  y_accelerometer  x_accelerometer  \\\n",
       "0            0.018845        -0.462222        -0.368679        -0.634270   \n",
       "1            0.028829        -0.605616        -0.620192        -0.543474   \n",
       "2            0.038814        -0.822008        -0.454557        -0.869079   \n",
       "3            0.048798        -1.715980        -0.542211        -1.278268   \n",
       "4            0.058782        -2.304195        -0.044734        -1.466343   \n",
       "...               ...              ...              ...              ...   \n",
       "5156        51.499675        -0.606824         0.025912        -0.293244   \n",
       "5157        51.509660        -0.618310         0.025874        -0.135084   \n",
       "5158        51.519644        -0.727742        -0.061432         0.085160   \n",
       "5159        51.529629        -0.635359        -0.233826         0.323464   \n",
       "5160        51.539613        -0.205602        -0.248753         0.272006   \n",
       "\n",
       "      z_gravity  y_gravity  x_gravity  z_gyroscope  y_gyroscope  x_gyroscope  \n",
       "0     -9.118624  -3.603003  -0.198613     0.103786    -0.146539     0.358608  \n",
       "1     -9.102573  -3.641936  -0.223269     0.061494    -0.329839     0.508239  \n",
       "2     -9.078464  -3.699083  -0.262031     0.021028    -0.478320     0.774517  \n",
       "3     -9.041318  -3.785458  -0.308639     0.026203    -0.532712     1.140537  \n",
       "4     -8.988506  -3.905100  -0.356842     0.082989    -0.477392     1.519668  \n",
       "...         ...        ...        ...          ...          ...          ...  \n",
       "5156  -8.390137  -5.030543   0.685295     0.013766     0.387161     0.390092  \n",
       "5157  -8.370122  -5.060581   0.708495    -0.010532     0.170958     0.322333  \n",
       "5158  -8.356308  -5.082959   0.711361    -0.032190    -0.140191     0.219038  \n",
       "5159  -8.348583  -5.098061   0.693773    -0.017748    -0.272743     0.157781  \n",
       "5160  -8.342745  -5.109919   0.676547     0.010838    -0.121989     0.129650  \n",
       "\n",
       "[5161 rows x 10 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#// BEGIN_TODO  Read, merge and select data\n",
    "\n",
    "accelerometer = pd.read_csv('../Data/Smooth/1/Accelerometer.csv')\n",
    "gravity = pd.read_csv('../Data/Smooth/1/Gravity.csv')\n",
    "gyroscope = pd.read_csv('../Data/Smooth/1/Gyroscope.csv')\n",
    "\n",
    "# remove time column\n",
    "accelerometer.drop(columns='time', inplace=True)\n",
    "gravity.drop(columns='time', inplace=True)\n",
    "gyroscope.drop(columns='time', inplace=True)\n",
    "\n",
    "# prefix column name\n",
    "gyroscope.rename(columns={'z':'z_gyroscope', 'y':'y_gyroscope', 'x':'x_gyroscope'}, inplace=True)\n",
    "\n",
    "data = pd.merge_asof(accelerometer, gravity, on='seconds_elapsed', suffixes=('_accelerometer', '_gravity'))\n",
    "\n",
    "data = pd.merge_asof(data, gyroscope, on='seconds_elapsed')\n",
    "data\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['seconds_elapseda', 'za', 'ya', 'xa'], dtype='object')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gyroscope.columns+'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 2.1\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to restrict our focus to bumpy lane detection, we would like to trim the recorded segment. In this way we can remove the movements corresponding to starting and stopping the sensor logger app. \n",
    "\n",
    "---\n",
    "---\n",
    "### Exercise 2.2: Trim data\n",
    "Remove the first and last 5 seconds of the recordings for this purpose and save this trimmed data frame to `data_trimmed`. Make sure that your code works for a data frame containing an arbitrary number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Trim data\n",
    "\n",
    "\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trimmed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trimmed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 2.2\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 2.3: Normalize recordings\n",
    "For improved processing, the recordings should be normalized. Normalize the recordings by subtracting its mean and by then dividing by its standard deviation. Perform this normalization for each column individually. Save your normalized data in the the data frame `data_norm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Normalize recordings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 2.3\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot recordings\n",
    "ex2_plot_data(data_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Feature extraction\n",
    "The current data is not yet suited for building the detector. Based on the measurements at a specific point of time, it is difficult to determine whether the bike lane is smooth or bumpy. Instead, it would be more appropriate to perform the detection over _segments_ of time. In this part you will extract features that will be used for the detection. First all collected datasets will be processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 3.1: Merge all datasets\n",
    "Before starting the feature extraction, merge all the preprocessed datasets obtained in the protocol. You will need to load all recording, and again perform all preprocessing steps of Part 2 for the individual recordings. Make sure your code adheres to proper coding standards (make it look nice, don't copy part 2 15 times). Save your merged data in the the data frame `data_merged`, in addition, save merged data of bumpy lanes as `data_bumpy` and data of smooth lanes as `data_smooth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Normalize recordings\n",
    "\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bumpy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_smooth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot recordings of data_merged\n",
    "ex2_plot_data(data_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot recordings of data_smooth\n",
    "ex2_plot_data(data_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot recordings of data_bumpy\n",
    "ex2_plot_data(data_bumpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 3.1\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 3.2: Convert dataframe to matrix\n",
    "In order to extract features from the recordings, convert the dataframe `data_merged` to a Numpy matrix called `mat_merged`. The matrix should have dimensions (_nr of time points_, _nr or different recordings_). Make sure that you remove the _seconds_elapsed_ column, as this does not yield any useful information for the ;ane detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO  Convert to matrix\n",
    "\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 3.2\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataframe has been converted to a matrix, it can be split into different overlapping segments, of which we can extract features.\n",
    "\n",
    "As a starting point we will specify features as the mean value of a segment, its standard deviation, its minimum and its maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 3.3: Processing segments\n",
    "Create a function `Y = process_segment(mat, fun, L, K)` that processes time segments of your matrix. The argument `fun` specifies the operation to be performed on the segment and its value comes from the set `[\"mean\", \"std\", \"minimum\", \"maximum\"]`. `L` specifies the segment length and `K` specifies the number of samples overlap between segments. The function should return a matrix `Y` with dimensions (_nr of segments_, _nr of different recordings_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Process segments\n",
    "\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_mean_1 = process_segment(mat_merged, \"mean\", 20, 10)\n",
    "Y_std_1 = process_segment(mat_merged, \"std\", 20, 10)\n",
    "Y_minimum_1 = process_segment(mat_merged, \"minimum\", 20, 10)\n",
    "Y_maximum_1 = process_segment(mat_merged, \"maximum\", 20, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 3.3\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 3.4: Concatenate features\n",
    "Now that you have computed some features of the recordings, it becomes necessary to combine them. Create the matrix `combined_features` which concatenates the above results along the appropriate axis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Concatenate features\n",
    "\n",
    "\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 3.4\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.5: preprocessing smooth and bumpy dataframes\n",
    "perform the same preprocessing steps on your smooth and bumpy dataframe. By the end of the exercise you should output two additional matrices; `smooth_features` and `combined_features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Concatenate features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dimensions of combined features:\", combined_features.shape)\n",
    "print(\"Dimensions of smooth features:\", smooth_features.shape)\n",
    "print(\"Dimensions of bumpy features:\", bumpy_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 3.5\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Data Compression\n",
    "In this part of the assignment you will implement the data compression techniques you have learned during the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4.1: Principal component analysis\n",
    "\n",
    "Recorded data is often high-dimensional, leading to a large computational burden and limiting real-time data processing. Quite often we would therefore like to compress the data, such that it uses less memory. The creation of .zip-files on your computer is an example of data compression. In this part we will look at one method that achieves data compression, namely Principal component analysis (PCA). PCA is a useful technique for analyzing high-dimensional data, and compressing this data for storage or processing purposes. PCA aims to fit an orthogonal coordinate system to a dataset that best captures the variance or spread of the data. This approach is very closely related to the eigendecomposition of a matrix.\n",
    "\n",
    "PCA aims to find the orthogonal directions of most variance of the dataset. Therefore it first computes the covariance matrix of the dataset, which can be estimated as\n",
    "$$\\Sigma = \\mathrm{Cov}[{\\bf{x}}] \\approx \\frac{1}{N-1} \\sum_{n=1}^N ({\\bf{x}}_n - \\mathrm{E}[{\\bf{x}}])({\\bf{x}}_n - \\mathrm{E}[{\\bf{x}}])^\\top,$$\n",
    "where ${\\bf{x}}_n$ is the $n$-th data sample and where $\\mathrm{E}[{\\bf{x}}]$ represents the expected value, or mean, which can be estimated as\n",
    "$$\\mathrm{E}[{\\bf{x}}] \\approx \\frac{1}{N} \\sum_{n=1}^N {\\bf{x}}_n.$$\n",
    "The covariance matrix captures the variances of the individual elements/features in ${\\bf{x}}_n$ and the covariances between elements/features. \n",
    "\n",
    "Based on the obtained covariance matrix, you will perform an eigendecomposition. An eigenvalue decomposition finds a set of eigenvectors and corresponding eigenvalues. An eigenvector ${\\bf{v}}$ and corresponding eigenvalue ${\\lambda}$ of some square matrix $A$ satisfy the equation\n",
    "$$A {\\bf{v}} = \\lambda {\\bf{v}}.$$\n",
    "In matrix notation, this can be written as\n",
    "$$A Q = Q\\Lambda,$$\n",
    "where $Q$ is an orthonormal matrix where each column represents an eigenvector. Orthonormal refers to the individual eigenvectors being of unit length, and perpendicular to eachother. This matrix satisfies the useful properties $QQ^\\top = Q^\\top Q = I$ and $Q^\\top = Q^{-1}$. $\\Lambda$ is a diagonal matrix, whose diagonal contains the eigenvalues of $A$. The matrix $A$ can therefore be decomposed as \n",
    "$$A = Q\\Lambda Q^\\top$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 4.1: Compute eigenvalues and eigenvectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `compute_eigen(X)` which computes the eigenvalues and eigenvectors of the covariance matrix of some toy dataset `X`. The dataset is a matrix of shape ($N\\times M$), where $N$ denotes the number of data samples and $M$ the dimensionality of the features. The function should return a vector of length $M$ containing the eigenvalues and a matrix of shape ($M\\times M$) containing the corresponding eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Compute eigenvalues and eigenvectors\n",
    "\n",
    "\n",
    "    \n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "X = ex11_generate_data()\n",
    "\n",
    "# compute eigenvalues and eigenvectors\n",
    "eigvals, eigvecs = compute_eigen(X)\n",
    "\n",
    "# plot data\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X[:,0], X[:,1], alpha=0.2)\n",
    "plot_eigen(np.mean(X, axis=0), eigvals, eigvecs, plt.gca(), width=0.1, color=\"r\")\n",
    "plt.axis(\"equal\")\n",
    "plt.grid()\n",
    "\n",
    "# print eigenvalues and eigenvectors\n",
    "eigvals, eigvecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the computed eigenvalues, eigenvectors and the visualization and answer the questions below:\n",
    "- In what direction does the first eigenvector point?\n",
    "- How can the eigenvalue be interpreted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BEGIN_TODO Interpretation of eigenvalues and eigenvectors`\n",
    "\n",
    "Answer: \n",
    "\n",
    "\n",
    "`END_TODO `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 4.1\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the computed eigendecomposition, the original dataset can be transformed such that its new mean coincides with the origin (0,0) and that the new eigenvectors specify Euclidean standard basis vectors (i.e. the new covariance matrix is diagonal). The first step can be obtained by subtracting the mean from the dataset. The origin-centered dataset $\\tilde{X}$ now is centered in the origin and therefore the covariance matrix can be estimated as \n",
    "$$ \\tilde{\\Sigma} = \\mathrm{Cov}[\\tilde{\\bf{x}}] \\approx \\frac{1}{N-1} \\tilde{X}^\\top \\tilde{X}\\qquad\\qquad \\text{if }\\mathrm{E}[\\tilde{{\\bf{x}}}] = {\\bf{0}}$$\n",
    "with the corresponding eigendecomposition $\\tilde{\\Sigma} = \\tilde{Q} \\tilde{\\Lambda} \\tilde{Q}^\\top$.\n",
    "\n",
    "If we multiply $\\tilde{X}$ with $\\tilde{Q}$ to get the transformed dataset $Y=\\tilde{X}\\tilde{Q}$, we observe that the new covariance matrix becomes\n",
    "$$\\mathrm{Cov}[{\\bf{y}}] \\approx \\frac{1}{N-1} Y^\\top Y = \\frac{1}{N-1}\\tilde{Q}^\\top \\tilde{X}^\\top \\tilde{X} \\tilde{Q} = \\tilde{Q}^\\top\\tilde{\\Sigma}\\tilde{Q} = \\tilde{Q}^\\top\\tilde{Q} \\tilde{\\Lambda} \\tilde{Q}^\\top\\tilde{Q} = \\tilde{\\Lambda},$$\n",
    "which is diagonal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 4.2: Data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `transform_PCA(X, mean, eigvecs)` which translates some dataset `X` to be centered in the origin, and rotates it, such that its new covariance matrix is diagonal. $X$ is of shape ($N\\times M$), where $N$ denotes the number of data samples and $M$ the dimensionality of the features. The function should return the transformed dataset of shape ($N\\times M$). Also create the function `inversetransform_PCA(X, mean, eigvecs)` which performs the inverse transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Transform data to PCA space\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Transform data back from PCA space\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#// END_TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data\n",
    "m = np.mean(X, axis=0)\n",
    "eigvals, eigvecs = compute_eigen(X)\n",
    "Y = transform_PCA(X, m, eigvecs)\n",
    "Z = inversetransform_PCA(Y, m, eigvecs)\n",
    "\n",
    "# plot transformed data\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10,5))\n",
    "ax[0].scatter(Y[:,0], Y[:,1], alpha=0.2)\n",
    "eigvalsY, eigvecsY = compute_eigen(Y)\n",
    "plot_eigen(np.mean(Y, axis=0), eigvalsY, eigvecsY, ax[0], width=0.1, color=\"r\")\n",
    "ax[1].scatter(Z[:,0], Z[:,1], alpha=0.2)\n",
    "eigvalsZ, eigvecsZ = compute_eigen(Z)\n",
    "plot_eigen(np.mean(Z, axis=0), eigvalsZ, eigvecsZ, ax[1], width=0.1, color=\"r\")\n",
    "ax[0].axis(\"equal\"), ax[1].axis(\"equal\")\n",
    "ax[0].grid(), ax[1].grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 4.2\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far PCA has only been discussed for a toy example. Let's now apply it to high-dimensional data. We will load a dataset containing 400 images of faces. These grayscale images are of size (64 $\\times$ 64) and therefore contain 4096 pixels. In order to process the images, they have been flattened into vectors, which are appended to create a matrix containing 400 images. Below we have plotted the first 100 images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ex13_generate_data()\n",
    "plot_faces(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 4.3: Principal components analysis\n",
    "Compute the eigenvalues and vectors of the faces dataset. Plot the first 350 eigenvalues with both a normal as log-scaling on the y-axis.\n",
    "\n",
    "> Note: Since the dataset only contains 400 images, the covariance matrix of size (4096 $\\times$ 4096) is not positive definite (although it should be in theory). Therefore the eigenvalues > 400 are basically useless, however, they are still computed as imaginary quantities. You can plot the real or absolute values of the eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Plot eigenvalues\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the created plots, what do you observe? How could this be useful for data compression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#// BEGIN_TODO  What do you observe?`\n",
    "\n",
    "\n",
    "`#// END_TODO`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 4.3\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use PCA for data compression. Instead of performing the transform to the PCA space with the entire eigenvector matrix with shape (4096 $\\times$ 4096), we only use the $K$ eigenvectors corresponding with the $K$ largest eigenvalues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 4.4: Data compression\n",
    "Compress the faces dataset using PCA in a matrix of shape (400 $\\times$ $K$) and then decompress the data and plot the faces using the `plot_faces()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO Data compression\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#// END_TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the results. What do you observe if you change $K$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#// BEGIN_TODO What do you observe?`\n",
    "\n",
    "\n",
    "`#// END_TODO`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 4.4\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 4.5: PCA implementation on biking data\n",
    "During this part you can make use of the sklearn (Scikit learn) package. This package offers some benefits over the handwritten clustering functions. These algorithms are numerically stable and better optimized to run on large data sets. choose 2 principal components for this exercise. Store the compressed data of combined_features as pca_combined, store the compressed data for smooth_features as pca_smooth, store the compressed data for bumpy features as pca_bumpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO PCA with sklearn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#// END_TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a figure with 3 subplots in a single row\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot PCA for combined features\n",
    "axs[0].scatter(pca_combined[:, 0], pca_combined[:, 1], c='blue', edgecolor='k')\n",
    "axs[0].set_title('PCA - Combined')\n",
    "axs[0].set_xlabel('Principal Component 1')\n",
    "axs[0].set_ylabel('Principal Component 2')\n",
    "\n",
    "# Plot PCA for smooth features\n",
    "axs[1].scatter(pca_smooth[:, 0], pca_smooth[:, 1], c='green', edgecolor='k')\n",
    "axs[1].set_title('PCA - Smooth')\n",
    "axs[1].set_xlabel('Principal Component 1')\n",
    "axs[1].set_ylabel('Principal Component 2')\n",
    "\n",
    "# Plot PCA for bumpy features\n",
    "axs[2].scatter(pca_bumpy[:, 0], pca_bumpy[:, 1], c='red', edgecolor='k')\n",
    "axs[2].set_title('PCA - Bumpy')\n",
    "axs[2].set_xlabel('Principal Component 1')\n",
    "axs[2].set_ylabel('Principal Component 2')\n",
    "\n",
    "# Adjust layout so plots don’t overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we are choosing 2 components, how many principal components would you consider? Justify your answer quantitatively and qualitatively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#// BEGIN_TODO  PCA with sklearn/qualitative`\n",
    "\n",
    "\n",
    "`#// END_TODO`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO  PCA with sklearn/quantitative\n",
    "\n",
    "#// END_TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 4.5\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4.2: Independent component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach of finding the most important components in a dataset is independent component analysis (ICA). Below you will use it to analyze a toy data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 4.6: Limitations PCA\n",
    "Have a look at the dataset below. Would PCA be a good approach for finding the components of highest variance? Please motivate your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ex21_generate_data()\n",
    "plt.scatter(X[:,0], X[:,1], alpha=0.1)\n",
    "plt.grid(True)\n",
    "plt.axis(\"equal\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#// BEGIN_TODO Limitations PCA`\n",
    "\n",
    "\n",
    "`#// END_TODO`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 4.6\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: From this moment onwards you can use the `FastICA` function from `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 4.7: PCA versus ICA\n",
    "Use the `PCA` and `FastICA` functions from `sklearn` to create the objects `pca_object` and `ica_object`, each with two components. Fit these objects to the dataset and transform the dataset. Save the transformed dataset into the variables `data_transformed_pca` and `data_transformed_ica`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO PCA and ICA\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#// END_TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(ncols=3, figsize=(15,5))\n",
    "ax[0].scatter(X[:,0], X[:,1], alpha=0.1)\n",
    "ax[1].scatter(data_transformed_pca[:,0], data_transformed_pca[:,1], alpha=0.1)\n",
    "ax[2].scatter(data_transformed_ica[:,0], data_transformed_ica[:,1], alpha=0.1)\n",
    "plot_pca(ax[0], pca_object, np.mean(X, axis=0))\n",
    "plot_ica(ax[0], ica_object, np.mean(X, axis=0))\n",
    "ax[0].grid(True), ax[1].grid(True), ax[2].grid(True)\n",
    "ax[0].axis('equal'), ax[1].axis('equal'), ax[2].axis('equal')\n",
    "ax[0].set_title(\"original data\"), ax[1].set_title(\"transformed data (PCA)\"), ax[2].set_title(\"transformed data (ICA)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run your code a couple of times. What do you observe? Which method works best for this data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#// BEGIN_TODO PCA versus ICA`\n",
    "\n",
    "\n",
    "`#// END_TODO`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 4.7\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Make sure to restart this notebook and to rerun all cells before submission to check whether all code runs properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Assignment 1a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
